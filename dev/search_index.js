var documenterSearchIndex = {"docs":
[{"location":"tutorials/#Tutorials","page":"Tutorials","title":"Tutorials","text":"","category":"section"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"We suggest getting started with the tutorials.","category":"page"},{"location":"tutorials/#Quickstart","page":"Tutorials","title":"Quickstart","text":"","category":"section"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"A basic demonstration on how to find adversarial examples for a pre-trained example network on the MNIST dataset.","category":"page"},{"location":"tutorials/#Importing-your-own-neural-net","page":"Tutorials","title":"Importing your own neural net","text":"","category":"section"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Explains how to import your own network for verification.","category":"page"},{"location":"tutorials/#Finding-adversarial-examples,-in-depth","page":"Tutorials","title":"Finding adversarial examples, in depth","text":"","category":"section"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Discusses the various parameters you can select for find_adversarial_example. We explain how to","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Better specify targeted labels for the perturbed image (including multiple targeted labels)\nHave more precise control over the activations in the output layer\nRestrict the family of perturbations (for example to the blurring perturbations discussed in our paper)\nSelect whether you want to minimize the L_1, L_2 or L_infty norm of the perturbation.\nModify the amount of time dedicated to building the model (by selecting the tightening_algorithm, and/or passing in custom tightening_options).","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"For Gurobi, we show how to specify optimizer settings to:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Mute output\nTerminate early if:\nA time limit is reached\nLower bounds on robustness are proved (that is, we prove that no adversarial example can exist closer than some threshold)\nAn adversarial example is found that is closer to the input than expected\nThe gap between the upper and lower objective bounds falls below a selected threshold","category":"page"},{"location":"tutorials/#Interpreting-the-output-of-find_adversarial_example","page":"Tutorials","title":"Interpreting the output of find_adversarial_example","text":"","category":"section"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Walks you through the output dictionary produced by a call to find_adversarial_example.","category":"page"},{"location":"tutorials/#Managing-log-output","page":"Tutorials","title":"Managing log output","text":"","category":"section"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Explains how to get more granular log settings and to write log output to file.","category":"page"},{"location":"net_components/layers/#Layers","page":"Layers","title":"Layers","text":"","category":"section"},{"location":"net_components/layers/","page":"Layers","title":"Layers","text":"Each layer in the neural net corresponds to a struct that simultaneously specifies: 1) the operation being carried out in the layer (recorded in the type of the struct) and 2) the parameters for the operation (recorded in the values of the fields of the struct).","category":"page"},{"location":"net_components/layers/","page":"Layers","title":"Layers","text":"When we pass an input array of real numbers to a layer struct, we get an output array of real numbers that is the result of the layer operating on the input.","category":"page"},{"location":"net_components/layers/","page":"Layers","title":"Layers","text":"Conversely, when we pass an input array of JuMP variables, we get an output array of JuMP variables, with the appropriate mixed-integer constraints (as determined by the layer) imposed between the input and output.","category":"page"},{"location":"net_components/layers/#Index","page":"Layers","title":"Index","text":"","category":"section"},{"location":"net_components/layers/","page":"Layers","title":"Layers","text":"Pages   = [\"layers.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"net_components/layers/#Public-Interface","page":"Layers","title":"Public Interface","text":"","category":"section"},{"location":"net_components/layers/","page":"Layers","title":"Layers","text":"Modules = [MIPVerify]\nOrder   = [:function, :type]\nPages   = [\n    \"net_components/layers/conv2d.jl\",\n    \"net_components/layers/flatten.jl\",\n    \"net_components/layers/linear.jl\",\n    \"net_components/layers/masked_relu.jl\",\n    \"net_components/layers/normalize.jl\",\n    \"net_components/layers/pool.jl\",\n    \"net_components/layers/relu.jl\",\n    \"net_components/layers/skip_unit.jl\",\n    \"net_components/layers/zero.jl\",\n    ]\nPrivate = false","category":"page"},{"location":"net_components/layers/#MIPVerify.Conv2d","page":"Layers","title":"MIPVerify.Conv2d","text":"struct Conv2d{T<:Union{Real, JuMP.VariableRef, JuMP.AffExpr}, U<:Union{Real, JuMP.VariableRef, JuMP.AffExpr}, V<:Integer} <: Layer\n\nRepresents 2-D convolution operation.\n\np(x) is shorthand for conv2d(x, p) when p is an instance of Conv2d.\n\nFields:\n\nfilter\nbias\nstride\npadding\n\n\n\n\n\n","category":"type"},{"location":"net_components/layers/#MIPVerify.Conv2d-Union{Tuple{Array{T, 4}}, Tuple{T}} where T<:Union{Real, JuMP.VariableRef, JuMP.AffExpr}","page":"Layers","title":"MIPVerify.Conv2d","text":"Conv2d(filter)\n\n\nConvenience function to create a Conv2d struct with the specified filter and zero bias.\n\n\n\n\n\n","category":"method"},{"location":"net_components/layers/#MIPVerify.Flatten","page":"Layers","title":"MIPVerify.Flatten","text":"struct Flatten{T<:Integer} <: Layer\n\nRepresents a flattening operation.\n\np(x) is shorthand for permute_and_flatten(x, p.perm) when p is an instance of Flatten.\n\nFields:\n\nn_dim\nperm\n\n\n\n\n\n","category":"type"},{"location":"net_components/layers/#MIPVerify.Linear","page":"Layers","title":"MIPVerify.Linear","text":"struct Linear{T<:Real, U<:Real} <: Layer\n\nRepresents matrix multiplication.\n\np(x) is shorthand for matmul(x, p) when p is an instance of Linear.\n\nFields:\n\nmatrix\nbias\n\n\n\n\n\n","category":"type"},{"location":"net_components/layers/#MIPVerify.MaskedReLU","page":"Layers","title":"MIPVerify.MaskedReLU","text":"struct MaskedReLU{T<:Real} <: Layer\n\nRepresents a masked ReLU activation, with mask controlling how the ReLU is applied to each output.\n\np(x) is shorthand for masked_relu(x, p.mask) when p is an instance of MaskedReLU.\n\nFields:\n\nmask\ntightening_algorithm\n\n\n\n\n\n","category":"type"},{"location":"net_components/layers/#MIPVerify.Normalize","page":"Layers","title":"MIPVerify.Normalize","text":"struct Normalize <: Layer\n\nRepresents a Normalization operation.\n\n\n\n\n\n","category":"type"},{"location":"net_components/layers/#MIPVerify.MaxPool-Union{Tuple{Tuple{Vararg{Int64, N}}}, Tuple{N}} where N","page":"Layers","title":"MIPVerify.MaxPool","text":"MaxPool(strides)\n\n\nConvenience function to create a Pool struct for max-pooling.\n\n\n\n\n\n","category":"method"},{"location":"net_components/layers/#MIPVerify.Pool","page":"Layers","title":"MIPVerify.Pool","text":"struct Pool{N} <: Layer\n\nRepresents a pooling operation.\n\np(x) is shorthand for pool(x, p) when p is an instance of Pool.\n\nFields:\n\nstrides\npooling_function\n\n\n\n\n\n","category":"type"},{"location":"net_components/layers/#MIPVerify.ReLU","page":"Layers","title":"MIPVerify.ReLU","text":"struct ReLU <: Layer\n\nRepresents a ReLU operation.\n\np(x) is shorthand for relu(x) when p is an instance of ReLU.\n\n\n\n\n\n","category":"type"},{"location":"net_components/layers/#MIPVerify.SkipBlock","page":"Layers","title":"MIPVerify.SkipBlock","text":"struct SkipBlock <: Layer\n\nTODO (vtjeng)\n\nFields:\n\nlayers\n\n\n\n\n\n","category":"type"},{"location":"net_components/layers/#MIPVerify.Zero","page":"Layers","title":"MIPVerify.Zero","text":"struct Zero <: Layer\n\nAlways outputs exactly zero.\n\n\n\n\n\n","category":"type"},{"location":"net_components/layers/#Internal","page":"Layers","title":"Internal","text":"","category":"section"},{"location":"net_components/layers/","page":"Layers","title":"Layers","text":"Modules = [MIPVerify]\nOrder   = [:function, :type]\nPages   = [\n    \"net_components/layers/conv2d.jl\",\n    \"net_components/layers/flatten.jl\",\n    \"net_components/layers/linear.jl\",\n    \"net_components/layers/masked_relu.jl\",\n    \"net_components/layers/normalize.jl\",\n    \"net_components/layers/pool.jl\",\n    \"net_components/layers/relu.jl\",\n    \"net_components/layers/skip_unit.jl\",\n    \"net_components/layers/zero.jl\",\n    ]\nPublic  = false","category":"page"},{"location":"net_components/layers/#MIPVerify.conv2d-Union{Tuple{V}, Tuple{U}, Tuple{T}, Tuple{Array{T, 4}, Conv2d{U, V}}} where {T<:Union{Real, JuMP.VariableRef, JuMP.AffExpr}, U<:Union{Real, JuMP.VariableRef, JuMP.AffExpr}, V<:Union{Real, JuMP.VariableRef, JuMP.AffExpr}}","page":"Layers","title":"MIPVerify.conv2d","text":"conv2d(input, params)\n\n\nComputes the result of convolving input with the filter and bias stored in params.\n\nMirrors tf.nn.conv2d from the tensorflow package, with strides = [1, params.stride, params.stride, 1].\n\nSupports three types of padding:\n\n'same':  Specify via SamePadding(). Padding is added so that the output has the same size as the input.\n'valid': Specify via FixedPadding(). No padding is added.\n'fixed': Specify via:\nA single integer, interpreted as padding for both axes\nA tuple of two integers, interpreted as (ypadding, xpadding)\nA tuple of four integers, interpreted as (top, bottom, left, right)\n\nThrows\n\nAssertionError if input and filter are not compatible.\n\n\n\n\n\n","category":"method"},{"location":"net_components/layers/#MIPVerify.permute_and_flatten-Union{Tuple{U}, Tuple{N}, Tuple{T}, Tuple{Array{T, N}, AbstractArray{U}}} where {T, N, U<:Integer}","page":"Layers","title":"MIPVerify.permute_and_flatten","text":"Permute dimensions of array in specified order, then flattens the array.\n\n\n\n\n\n","category":"method"},{"location":"net_components/layers/#MIPVerify.matmul-Tuple{Vector{<:Real}, Linear}","page":"Layers","title":"MIPVerify.matmul","text":"matmul(x, params)\n\n\nComputes the result of pre-multiplying x by the transpose of params.matrix and adding params.bias.\n\n\n\n\n\n","category":"method"},{"location":"net_components/layers/#MIPVerify.matmul-Union{Tuple{V}, Tuple{U}, Tuple{T}, Tuple{Vector{T}, Linear{U, V}}} where {T<:Union{JuMP.VariableRef, JuMP.AffExpr}, U<:Real, V<:Real}","page":"Layers","title":"MIPVerify.matmul","text":"matmul(x, params)\n\n\nComputes the result of pre-multiplying x by the transpose of params.matrix and adding params.bias. We write the computation out by hand when working with JuMPLinearType so that we are able to simplify the output as the computation is carried out.\n\n\n\n\n\n","category":"method"},{"location":"net_components/layers/#MIPVerify.getoutputsize-Union{Tuple{N}, Tuple{T}, Tuple{AbstractArray{T, N}, Tuple{Vararg{Int64, N}}}} where {T, N}","page":"Layers","title":"MIPVerify.getoutputsize","text":"getoutputsize(input_array, strides)\n\n\nFor pooling operations on an array, returns the expected size of the output array.\n\n\n\n\n\n","category":"method"},{"location":"net_components/layers/#MIPVerify.getpoolview-Union{Tuple{N}, Tuple{T}, Tuple{AbstractArray{T, N}, Tuple{Vararg{Int64, N}}, Tuple{Vararg{Int64, N}}}} where {T, N}","page":"Layers","title":"MIPVerify.getpoolview","text":"getpoolview(input_array, strides, output_index)\n\n\nFor pooling operations on an array, returns a view of the parent array corresponding to the output_index in the output array.\n\n\n\n\n\n","category":"method"},{"location":"net_components/layers/#MIPVerify.getsliceindex-Tuple{Integer, Integer, Integer}","page":"Layers","title":"MIPVerify.getsliceindex","text":"getsliceindex(input_array_size, stride, output_index)\n\n\nFor pooling operations on an array where a given element in the output array corresponds to equal-sized blocks in the input array, returns (for a given dimension) the index range in the input array corresponding to a particular index output_index in the output array.\n\nReturns an empty array if the output_index does not correspond to any input indices.\n\nArguments\n\nstride::Integer: the size of the operating blocks along the active    dimension.\n\n\n\n\n\n","category":"method"},{"location":"net_components/layers/#MIPVerify.pool-Union{Tuple{N}, Tuple{T}, Tuple{AbstractArray{T, N}, Pool{N}}} where {T<:Union{Real, JuMP.VariableRef, JuMP.AffExpr}, N}","page":"Layers","title":"MIPVerify.pool","text":"pool(input, params)\n\n\nComputes the result of applying the pooling function params.pooling_function to non-overlapping cells of input with sizes specified in params.strides.\n\n\n\n\n\n","category":"method"},{"location":"net_components/layers/#MIPVerify.poolmap-Union{Tuple{N}, Tuple{T}, Tuple{Function, AbstractArray{T, N}, Tuple{Vararg{Int64, N}}}} where {T, N}","page":"Layers","title":"MIPVerify.poolmap","text":"poolmap(f, input_array, strides)\n\n\nReturns output from applying f to subarrays of input_array, with the windows determined by the strides.\n\n\n\n\n\n","category":"method"},{"location":"net_components/nets/#Networks","page":"Networks","title":"Networks","text":"","category":"section"},{"location":"net_components/nets/","page":"Networks","title":"Networks","text":"Each network corresponds to an array of layers associated with a unique string identifier. The string identifier of the network is used to store cached models, so it's important to ensure that you don't re-use names!","category":"page"},{"location":"net_components/nets/#Public-Interface","page":"Networks","title":"Public Interface","text":"","category":"section"},{"location":"net_components/nets/","page":"Networks","title":"Networks","text":"Modules = [MIPVerify]\nOrder   = [:function, :type]\nPages   = [\n    \"net_components/nets/sequential.jl\",\n    \"net_components/nets/skip_sequential.jl\",\n    ]\nPrivate = false","category":"page"},{"location":"net_components/nets/#MIPVerify.Sequential","page":"Networks","title":"MIPVerify.Sequential","text":"struct Sequential <: NeuralNet\n\nRepresents a sequential (feed-forward) neural net, with layers ordered from input to output.\n\nFields:\n\nlayers\nUUID\n\n\n\n\n\n","category":"type"},{"location":"net_components/nets/#MIPVerify.SkipSequential","page":"Networks","title":"MIPVerify.SkipSequential","text":"struct SkipSequential <: NeuralNet\n\nRepresents a sequential (feed-forward) neural net, with layers ordered from input to output.\n\nFields:\n\nlayers\nUUID\n\n\n\n\n\n","category":"type"},{"location":"net_components/nets/#Internal","page":"Networks","title":"Internal","text":"","category":"section"},{"location":"net_components/nets/","page":"Networks","title":"Networks","text":"Modules = [MIPVerify]\nOrder   = [:function, :type]\nPages   = [\n    \"net_components/nets/sequential.jl\",\n    \"net_components/nets/skip_sequential.jl\",\n    ]\nPublic  = false","category":"page"},{"location":"net_components/core_ops/#Core-Operations","page":"Core Operations","title":"Core Operations","text":"","category":"section"},{"location":"net_components/core_ops/","page":"Core Operations","title":"Core Operations","text":"Our ability to cast the input-output constraints of a neural net to an efficient set of linear and integer constraints boils down to the following basic operations, over which the layers provide a convenient layer of abstraction.","category":"page"},{"location":"net_components/core_ops/#Index","page":"Core Operations","title":"Index","text":"","category":"section"},{"location":"net_components/core_ops/","page":"Core Operations","title":"Core Operations","text":"Pages   = [\"core_ops.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"net_components/core_ops/#Internal","page":"Core Operations","title":"Internal","text":"","category":"section"},{"location":"net_components/core_ops/","page":"Core Operations","title":"Core Operations","text":"Modules = [MIPVerify]\nOrder   = [:function, :type]\nPages   = [\n    \"net_components/core_ops.jl\"\n    ]\nPublic  = false","category":"page"},{"location":"net_components/core_ops/#MIPVerify.abs_ge-Tuple{Union{JuMP.VariableRef, JuMP.AffExpr}}","page":"Core Operations","title":"MIPVerify.abs_ge","text":"abs_ge(x)\n\n\nExpresses a one-sided absolute-value constraint: output is constrained to be at least as large as |x|.\n\nOnly use when you are minimizing over the output in the objective.\n\n\n\n\n\n","category":"method"},{"location":"net_components/core_ops/#MIPVerify.is_constant-Tuple{JuMP.AffExpr}","page":"Core Operations","title":"MIPVerify.is_constant","text":"is_constant(x)\n\n\nChecks whether a JuMPLinearType is constant (and thus has no model associated) with it. This can only be true if it is an affine expression with no stored variables.\n\n\n\n\n\n","category":"method"},{"location":"net_components/core_ops/#MIPVerify.lazy_tight_lowerbound-Tuple{Union{JuMP.VariableRef, JuMP.AffExpr}, Real}","page":"Core Operations","title":"MIPVerify.lazy_tight_lowerbound","text":"Calculates the lower_bound only if u is positive; otherwise, returns u (since we expect) the ReLU to be fixed to zero anyway.\n\n\n\n\n\n","category":"method"},{"location":"net_components/core_ops/#MIPVerify.masked_relu-Tuple{AbstractArray{<:Union{JuMP.VariableRef, JuMP.AffExpr}}, AbstractArray{<:Real}}","page":"Core Operations","title":"MIPVerify.masked_relu","text":"masked_relu(x, m; nta)\n\n\nExpresses a masked rectified-linearity constraint, with three possibilities depending on the value of the mask. Output is constrained to be:\n\n1) max(x, 0) if m=0,\n2) 0 if m<0\n3) x if m>0\n\n\n\n\n\n","category":"method"},{"location":"net_components/core_ops/#MIPVerify.maximum-Union{Tuple{AbstractArray{T}}, Tuple{T}} where T<:Union{JuMP.VariableRef, JuMP.AffExpr}","page":"Core Operations","title":"MIPVerify.maximum","text":"maximum(xs)\n\n\nExpresses a maximization constraint: output is constrained to be equal to max(xs).\n\n\n\n\n\n","category":"method"},{"location":"net_components/core_ops/#MIPVerify.maximum_ge-Union{Tuple{AbstractArray{T}}, Tuple{T}} where T<:Union{JuMP.VariableRef, JuMP.AffExpr}","page":"Core Operations","title":"MIPVerify.maximum_ge","text":"maximum_ge(xs)\n\n\nExpresses a one-sided maximization constraint: output is constrained to be at least max(xs).\n\nOnly use when you are minimizing over the output in the objective.\n\nNB: If all of xs are constant, we simply return the largest of them.\n\n\n\n\n\n","category":"method"},{"location":"net_components/core_ops/#MIPVerify.relax_integrality_context-Tuple{Any, JuMP.Model, Bool}","page":"Core Operations","title":"MIPVerify.relax_integrality_context","text":"relax_integrality_context(\n    f,\n    model,\n    should_relax_integrality\n)\n\n\nContext manager for running f on model. If should_relax_integrality is true, the  integrality constraints are relaxed before f is run and re-imposed after.\n\n\n\n\n\n","category":"method"},{"location":"net_components/core_ops/#MIPVerify.relu-Union{Tuple{AbstractArray{T}}, Tuple{T}} where T<:Union{JuMP.VariableRef, JuMP.AffExpr}","page":"Core Operations","title":"MIPVerify.relu","text":"relu(x)\nrelu(x; nta)\n\n\nExpresses a rectified-linearity constraint: output is constrained to be equal to max(x, 0).\n\n\n\n\n\n","category":"method"},{"location":"net_components/core_ops/#MIPVerify.set_max_indexes-Tuple{JuMP.Model, Vector{<:Union{JuMP.VariableRef, JuMP.AffExpr}}, Vector{<:Integer}}","page":"Core Operations","title":"MIPVerify.set_max_indexes","text":"set_max_indexes(model, xs, target_indexes; margin)\n\n\nImposes constraints ensuring that one of the elements at the targetindexes is (tied for) the largest element of the array x. More specifically, we require x[j] - x[i] ≥ margin for some `j ∈ targetindexesand for alli ∉ target_indexes`.\n\n\n\n\n\n","category":"method"},{"location":"net_components/core_ops/#MIPVerify.tight_bound-Tuple{Union{JuMP.VariableRef, JuMP.AffExpr}, Union{Nothing, MIPVerify.TighteningAlgorithm}, MIPVerify.BoundType, Real}","page":"Core Operations","title":"MIPVerify.tight_bound","text":"Calculates a tight bound of type bound_type on the variable x using the specified tightening algorithm nta.\n\nIf an upper bound is proven to be below cutoff, or a lower bound is proven to above cutoff, the algorithm returns early with whatever value was found.\n\n\n\n\n\n","category":"method"},{"location":"net_components/core_ops/#MIPVerify.tight_bound_helper-Tuple{JuMP.Model, MIPVerify.BoundType, Union{JuMP.VariableRef, JuMP.AffExpr}, Number}","page":"Core Operations","title":"MIPVerify.tight_bound_helper","text":"tight_bound_helper(m, bound_type, objective, b_0)\n\n\nOptimizes the value of objective based on bound_type, with b_0, computed via interval arithmetic, as a backup.\n\nIf an optimal solution is reached, we return the objective value. We also verify that the  objective found is better than the bound b_0 provided; if this is not the case, we throw an error.\nIf we reach the user-defined time limit, we compute the best objective bound found. We compare  this to b_0 and return the better result.\nFor all other solve statuses, we warn the user and report b_0.\n\n\n\n\n\n","category":"method"},{"location":"net_components/overview/#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"net_components/overview/","page":"Overview","title":"Overview","text":"A neural net consists of multiple layers, each of which (potentially) operates on input differently. We represent these objects with NeuralNet and Layer.","category":"page"},{"location":"net_components/overview/#Index","page":"Overview","title":"Index","text":"","category":"section"},{"location":"net_components/overview/","page":"Overview","title":"Overview","text":"Pages   = [\"overview.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"net_components/overview/#PublicInterface","page":"Overview","title":"PublicInterface","text":"","category":"section"},{"location":"net_components/overview/","page":"Overview","title":"Overview","text":"Modules = [MIPVerify]\nOrder   = [:function, :type]\nPages   = [\"net_components.jl\"]","category":"page"},{"location":"net_components/overview/#MIPVerify.chain-Tuple{Array{<:Union{Real, JuMP.VariableRef, JuMP.AffExpr}}, Vector{<:Layer}}","page":"Overview","title":"MIPVerify.chain","text":"An array of Layers is interpreted as that array of layer being applied to the input sequentially, starting from the leftmost layer. (In functional programming terms, this can be thought of as a sort of fold).\n\n\n\n\n\n","category":"method"},{"location":"net_components/overview/#MIPVerify.Layer","page":"Overview","title":"MIPVerify.Layer","text":"abstract type Layer\n\nSupertype for all types storing the parameters of each layer. Inherit from this to specify your own custom type of layer. Each implementation is expected to:\n\nImplement a callable specifying the output when any input of type JuMPReal is provided.\n\n\n\n\n\n","category":"type"},{"location":"net_components/overview/#MIPVerify.NeuralNet","page":"Overview","title":"MIPVerify.NeuralNet","text":"abstract type NeuralNet\n\nSupertype for all types storing the parameters of a neural net. Inherit from this to specify your own custom architecture. Each implementation is expected to:\n\nImplement a callable specifying the output when any input of type JuMPReal is provided\nHave a UUID field for the name of the neural network.\n\n\n\n\n\n","category":"type"},{"location":"utils/import_example_nets/#Example-Neural-Networks","page":"Example Neural Networks","title":"Example Neural Networks","text":"","category":"section"},{"location":"utils/import_example_nets/","page":"Example Neural Networks","title":"Example Neural Networks","text":"get_example_network_params imports the weights of networks verified in our paper, as well as other networks of interest, as NeuralNets that can immediately be verified via our tools.","category":"page"},{"location":"utils/import_example_nets/#Index","page":"Example Neural Networks","title":"Index","text":"","category":"section"},{"location":"utils/import_example_nets/","page":"Example Neural Networks","title":"Example Neural Networks","text":"Pages   = [\"import_example_nets.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"utils/import_example_nets/#Public-Interface","page":"Example Neural Networks","title":"Public Interface","text":"","category":"section"},{"location":"utils/import_example_nets/","page":"Example Neural Networks","title":"Example Neural Networks","text":"Modules = [MIPVerify]\nOrder   = [:function, :type]\nPages   = [\"utils/import_example_nets.jl\"]\nPrivate = false","category":"page"},{"location":"utils/import_example_nets/#MIPVerify.get_example_network_params-Tuple{String}","page":"Example Neural Networks","title":"MIPVerify.get_example_network_params","text":"get_example_network_params(name)\n\n\nMakes named example neural networks available as a NeuralNet object.\n\nArguments\n\nname::String: Name of example neural network. Options:\n'MNIST.n1':\nArchitecture: Two fully connected layers with 40 and 20 units.\nTraining: Trained regularly with no attempt to increase robustness.\n'MNIST.WK17a_linf0.1_authors'.\nArchitecture: Two convolutional layers (stride length 2) with 16 and 32 filters respectively (size 4 × 4 in both layers), followed by a fully-connected layer with 100 units.\nTraining: Network trained to be robust to attacks with l_infty norm at most 0.1 via method in Provable defenses against adversarial examples via the convex outer adversarial polytope. Is MNIST network for which results are reported in that paper.\n'MNIST.RSL18a_linf0.1_authors'.\nArchitecture: One fully connected layer with 500 units.\nTraining: Network trained to be robust to attacks with l_infty norm at most 0.1 via method in Certified Defenses against Adversarial Examples . Is MNIST network for which results are reported in that paper.\n\n\n\n\n\n","category":"method"},{"location":"utils/import_example_nets/#Internal","page":"Example Neural Networks","title":"Internal","text":"","category":"section"},{"location":"utils/import_example_nets/","page":"Example Neural Networks","title":"Example Neural Networks","text":"Modules = [MIPVerify]\nOrder   = [:function, :type]\nPages   = [\"utils/import_example_nets.jl\"]\nPublic  = false","category":"page"},{"location":"utils/import_weights/#Helpers-for-importing-individual-layers","page":"Helpers for importing individual layers","title":"Helpers for importing individual layers","text":"","category":"section"},{"location":"utils/import_weights/","page":"Helpers for importing individual layers","title":"Helpers for importing individual layers","text":"You're likely to want to import parameter values from your trained neural networks from outside of Julia. get_conv_params and get_matrix_params are helper functions enabling you to import individual layers.","category":"page"},{"location":"utils/import_weights/#Index","page":"Helpers for importing individual layers","title":"Index","text":"","category":"section"},{"location":"utils/import_weights/","page":"Helpers for importing individual layers","title":"Helpers for importing individual layers","text":"Pages   = [\"import_weights.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"utils/import_weights/#Public-Interface","page":"Helpers for importing individual layers","title":"Public Interface","text":"","category":"section"},{"location":"utils/import_weights/","page":"Helpers for importing individual layers","title":"Helpers for importing individual layers","text":"Modules = [MIPVerify]\nOrder   = [:function, :type]\nPages   = [\"utils/import_weights.jl\"]\nPrivate = false","category":"page"},{"location":"utils/import_weights/#MIPVerify.get_conv_params-Tuple{Dict{String}, String, NTuple{4, Int64}}","page":"Helpers for importing individual layers","title":"MIPVerify.get_conv_params","text":"get_conv_params(\n    param_dict,\n    layer_name,\n    expected_size;\n    matrix_name,\n    bias_name,\n    expected_stride,\n    padding\n)\n\n\nHelper function to import the parameters for a convolution layer from param_dict as a     Conv2d object.\n\nThe default format for the key is 'layer_name/weight' and 'layer_name/bias';     you can customize this by passing in the named arguments matrix_name and bias_name     respectively. The expected parameter names will then be 'layer_name/matrix_name'     and 'layer_name/bias_name'\n\nArguments\n\nparam_dict::Dict{String}: Dictionary mapping parameter names to array of weights   / biases.\nlayer_name::String: Identifies parameter in dictionary.\nexpected_size::NTuple{4, Int}: Tuple of length 4 corresponding to the expected size   of the weights of the layer.\n\n\n\n\n\n","category":"method"},{"location":"utils/import_weights/#MIPVerify.get_matrix_params-Tuple{Dict{String}, String, Tuple{Int64, Int64}}","page":"Helpers for importing individual layers","title":"MIPVerify.get_matrix_params","text":"get_matrix_params(\n    param_dict,\n    layer_name,\n    expected_size;\n    matrix_name,\n    bias_name\n)\n\n\nHelper function to import the parameters for a layer carrying out matrix multiplication     (e.g. fully connected layer / softmax layer) from param_dict as a     Linear object.\n\nThe default format for the key is 'layer_name/weight' and 'layer_name/bias';     you can customize this by passing in the named arguments matrix_name and bias_name     respectively. The expected parameter names will then be 'layer_name/matrix_name'     and 'layer_name/bias_name'\n\nArguments\n\nparam_dict::Dict{String}: Dictionary mapping parameter names to array of weights   / biases.\nlayer_name::String: Identifies parameter in dictionary.\nexpected_size::NTuple{2, Int}: Tuple of length 2 corresponding to the expected size  of the weights of the layer.\n\n\n\n\n\n","category":"method"},{"location":"utils/import_weights/#Internal","page":"Helpers for importing individual layers","title":"Internal","text":"","category":"section"},{"location":"utils/import_weights/","page":"Helpers for importing individual layers","title":"Helpers for importing individual layers","text":"Modules = [MIPVerify]\nOrder   = [:function, :type]\nPages   = [\"utils/import_weights.jl\"]\nPublic  = false","category":"page"},{"location":"finding_adversarial_examples/single_image/#Single-Image","page":"Single Image","title":"Single Image","text":"","category":"section"},{"location":"finding_adversarial_examples/single_image/","page":"Single Image","title":"Single Image","text":"find_adversarial_example finds the closest adversarial example to a given input image for a particular NeuralNet.","category":"page"},{"location":"finding_adversarial_examples/single_image/","page":"Single Image","title":"Single Image","text":"As a sanity check, we suggest that you verify that the NeuralNet imported achieves the expected performance on the test set. This can be done using frac_correct.","category":"page"},{"location":"finding_adversarial_examples/single_image/#Index","page":"Single Image","title":"Index","text":"","category":"section"},{"location":"finding_adversarial_examples/single_image/","page":"Single Image","title":"Single Image","text":"Pages   = [\"basic_usage.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"finding_adversarial_examples/single_image/#Public-Interface","page":"Single Image","title":"Public Interface","text":"","category":"section"},{"location":"finding_adversarial_examples/single_image/","page":"Single Image","title":"Single Image","text":"Modules = [MIPVerify]\nOrder   = [:function, :type]\nPages   = [\"MIPVerify.jl\"]\nPrivate = false","category":"page"},{"location":"finding_adversarial_examples/single_image/#MIPVerify.find_adversarial_example-Tuple{NeuralNet, Array{<:Real}, Union{Integer, Vector{<:Integer}}, Any, Dict}","page":"Single Image","title":"MIPVerify.find_adversarial_example","text":"find_adversarial_example(\n    nn,\n    input,\n    target_selection,\n    optimizer,\n    main_solve_options;\n    invert_target_selection,\n    pp,\n    norm_order,\n    adversarial_example_objective,\n    tightening_algorithm,\n    tightening_options,\n    solve_if_predicted_in_targeted\n)\n\n\nPerturbs input such that the network nn classifies the perturbed image in one of the categories identified by the indexes in target_selection.\n\nIMPORTANT: \n\ntarget_selection can include the correct label for input.\nIt is possible (particularly with the closest objective) to see 'ties' – that is, the perturbed input produces an output with two logits (one corresponding to a target category, and one corresponding to a non-target category) taking on the same maximal value. See the formal definition below for more; in particular, note that '≥' sign.\n\noptimizer is used to build and solve the MIP problem.\n\nThe output dictionary has keys :Model, :PerturbationFamily, :TargetIndexes, :SolveStatus, :Perturbation, :PerturbedInput, :Output. See the tutorial on what individual dictionary entries correspond to.\n\nFormal Definition: If there are a total of n categories, the (perturbed) output vector y=d[:Output]=d[:PerturbedInput] |> nn has length n. If :SolveStatus is feasible, we guarantee that y[j] - y[i] ≥ 0 for some j ∈ target_selection and for all i ∉ target_selection.\n\nKeyword Arguments:\n\ninvert_target_selection: Defaults to false. If true, sets target_selection to be its   complement.\npp: Defaults to UnrestrictedPerturbationFamily(). Determines the search space for adversarial examples.\nnorm_order: Defaults to 1. Determines the distance norm used to determine the distance from   the perturbed image to the original. Allowed options are 1 and Inf, and 2 if the   optimizer can solve MIQPs.\nadversarial_example_objective: Defaults to closest. Allowed options are closest or worst.\nclosest finds the closest adversarial example, as measured by the norm_order norm.\nworst finds the adversarial example with the largest gap between max(y[j) for j ∈ target_selection and max(y[i]) for all i ∉ target_selection.\ntightening_algorithm: Defaults to mip. Determines how we determine the upper and lower bounds   on input to each nonlinear unit. Allowed options are interval_arithmetic, lp, mip.\ninterval_arithmetic looks at the bounds on the output to the previous layer.\nlp solves an lp corresponding to the mip formulation, but with any integer constraints relaxed.\nmip solves the full mip formulation.\ntightening_options: Solver-specific options passed to optimizer when used to determine upper   and lower bounds for input to nonlinear units. Note that these are only used if the   tightening_algorithm is lp or mip (no solver is used when interval_arithmetic is used   to compute the bounds). Defaults for Gurobi and HiGHS to a time limit of 20s per solve, with   output suppressed.\nsolve_if_predicted_in_targeted: Defaults to true. The prediction that nn makes for the   unperturbed input can be determined efficiently. If the predicted index is one of the indexes   in target_selection, we can skip the relatively costly process of building the model for the   MIP problem since we already have an \"adversarial example\" –- namely, the input itself. We   continue build the model and solve the (trivial) MIP problem if and only if   solve_if_predicted_in_targeted is true.\n\n\n\n\n\n","category":"method"},{"location":"finding_adversarial_examples/single_image/#MIPVerify.frac_correct-Tuple{NeuralNet, MIPVerify.LabelledDataset, Integer}","page":"Single Image","title":"MIPVerify.frac_correct","text":"frac_correct(nn, dataset, num_samples)\n\n\nReturns the fraction of items the neural network correctly classifies of the first num_samples of the provided dataset. If there are fewer than num_samples items, we use all of the available samples.\n\nNamed Arguments:\n\nnn::NeuralNet: The parameters of the neural network.\ndataset::LabelledDataset:\nnum_samples::Integer: Number of samples to use.\n\n\n\n\n\n","category":"method"},{"location":"utils/import_datasets/#Datasets","page":"Datasets","title":"Datasets","text":"","category":"section"},{"location":"utils/import_datasets/","page":"Datasets","title":"Datasets","text":"For your convenience, the MNIST and CIFAR10 dataset is available as part of our package.","category":"page"},{"location":"utils/import_datasets/#Index","page":"Datasets","title":"Index","text":"","category":"section"},{"location":"utils/import_datasets/","page":"Datasets","title":"Datasets","text":"Pages   = [\"import_datasets.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"utils/import_datasets/#Public-Interface","page":"Datasets","title":"Public Interface","text":"","category":"section"},{"location":"utils/import_datasets/","page":"Datasets","title":"Datasets","text":"Modules = [MIPVerify]\nOrder   = [:function, :type]\nPages   = [\"utils/import_datasets.jl\"]\nPrivate = false","category":"page"},{"location":"utils/import_datasets/#MIPVerify.read_datasets-Tuple{String}","page":"Datasets","title":"MIPVerify.read_datasets","text":"read_datasets(name)\n\n\nMakes popular machine learning datasets available as a NamedTrainTestDataset.\n\nArguments\n\nname::String: name of machine learning dataset. Options:\nMNIST: The MNIST Database of handwritten digits. Pixel values in original dataset are provided as uint8 (0 to 255), but are scaled to range from 0 to 1 here.\nCIFAR10: Labelled subset in 10 classes of 80 million tiny images dataset. Pixel values in original dataset are provided as uint8 (0 to 255), but are scaled to range from 0 to 1 here.\n\n\n\n\n\n","category":"method"},{"location":"utils/import_datasets/#Internal","page":"Datasets","title":"Internal","text":"","category":"section"},{"location":"utils/import_datasets/","page":"Datasets","title":"Datasets","text":"Modules = [MIPVerify]\nOrder   = [:function, :type]\nPages   = [\"utils/import_datasets.jl\"]\nPublic  = false","category":"page"},{"location":"utils/import_datasets/#MIPVerify.LabelledImageDataset","page":"Datasets","title":"MIPVerify.LabelledImageDataset","text":"struct LabelledImageDataset{T<:Real, U<:Integer} <: MIPVerify.LabelledDataset\n\nDataset of images stored as a 4-dimensional array of size (num_samples, image_height, image_width, num_channels), with accompanying labels (sorted in the same order) of size num_samples.\n\n\n\n\n\n","category":"type"},{"location":"utils/import_datasets/#MIPVerify.NamedTrainTestDataset","page":"Datasets","title":"MIPVerify.NamedTrainTestDataset","text":"struct NamedTrainTestDataset{T<:MIPVerify.Dataset, U<:MIPVerify.Dataset} <: MIPVerify.Dataset\n\nNamed dataset containing a training set and a test set which are expected to contain the same kind of data.\n\nname: Name of dataset.\n\ntrain: Training set.\n\ntest: Test set.\n\n\n\n\n\n","category":"type"},{"location":"finding_adversarial_examples/batch_processing/#Batch-Processing","page":"Batch Processing","title":"Batch Processing","text":"","category":"section"},{"location":"finding_adversarial_examples/batch_processing/","page":"Batch Processing","title":"Batch Processing","text":"batch_find_untargeted_attack enables users to run find_adversarial_example for multiple samples from a single dataset, writing 1) a single summary .csv file for the dataset, with a row of summary results per sample, and 2) a file per sample containing the output dictionary from find_adversarial_example.","category":"page"},{"location":"finding_adversarial_examples/batch_processing/","page":"Batch Processing","title":"Batch Processing","text":"batch_find_untargeted_attack allows verification of a dataset to be resumed if the process is interrupted by intelligently determining whether to rerun find_adversarial_example on a sample based on the solve_rerun_option specified.","category":"page"},{"location":"finding_adversarial_examples/batch_processing/#Index","page":"Batch Processing","title":"Index","text":"","category":"section"},{"location":"finding_adversarial_examples/batch_processing/","page":"Batch Processing","title":"Batch Processing","text":"Pages   = [\"batch_processing.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"finding_adversarial_examples/batch_processing/#Public-Interface","page":"Batch Processing","title":"Public Interface","text":"","category":"section"},{"location":"finding_adversarial_examples/batch_processing/","page":"Batch Processing","title":"Batch Processing","text":"Modules = [MIPVerify]\nOrder   = [:function, :type]\nPages   = [\"batch_processing_helpers.jl\"]\nPrivate = false","category":"page"},{"location":"finding_adversarial_examples/batch_processing/#MIPVerify.batch_find_untargeted_attack-Tuple{NeuralNet, MIPVerify.LabelledDataset, AbstractArray{<:Integer}, Any, Dict}","page":"Batch Processing","title":"MIPVerify.batch_find_untargeted_attack","text":"batch_find_untargeted_attack(\n    nn,\n    dataset,\n    target_indices,\n    optimizer,\n    main_solve_options;\n    save_path,\n    solve_rerun_option,\n    pp,\n    norm_order,\n    tightening_algorithm,\n    tightening_options,\n    solve_if_predicted_in_targeted,\n    adversarial_example_objective\n)\n\n\nRuns find_adversarial_example for the specified neural network nn and dataset for samples identified by the target_indices, with the target labels for each sample set to the complement of the true label.\n\nIt creates a named directory in save_path, with the name summarizing\n\nthe name of the network in nn,\nthe perturbation family pp,\nthe norm_order\n\nWithin this directory, a summary of all the results is stored in summary.csv, and results from individual runs are stored in the subfolder run_results.\n\nThis functioned is designed so that it can be interrupted and restarted cleanly; it relies on the summary.csv file to determine what the results of previous runs are (so modifying this file manually can lead to unexpected behavior.)\n\nIf the summary file already contains a result for a given target index, the solve_rerun_option determines whether we rerun find_adversarial_example for this particular index.\n\noptimizer specifies the optimizer used to solve the MIP problem once it has been built and main_solve_options specifies the options that will be passed to the optimizer for the  main solve.\n\nNamed Arguments:\n\nsave_path: Directory where results will be saved. Defaults to current directory.\npp, norm_order, tightening_algorithm, tightening_options, solve_if_predicted_in_targeted are passed through to find_adversarial_example and have the same default values; see documentation for that function for more details.\nsolve_rerun_option::MIPVerify.SolveRerunOption: Options are never, always, resolve_ambiguous_cases, and refine_insecure_cases. See run_on_sample_for_untargeted_attack for more details.\n\n\n\n\n\n","category":"method"},{"location":"finding_adversarial_examples/batch_processing/#Internal","page":"Batch Processing","title":"Internal","text":"","category":"section"},{"location":"finding_adversarial_examples/batch_processing/","page":"Batch Processing","title":"Batch Processing","text":"Modules = [MIPVerify]\nOrder   = [:function, :type]\nPages   = [\"batch_processing_helpers.jl\"]\nPublic  = false","category":"page"},{"location":"finding_adversarial_examples/batch_processing/#MIPVerify.batch_find_targeted_attack-Tuple{NeuralNet, MIPVerify.LabelledDataset, AbstractArray{<:Integer}, Any, Dict}","page":"Batch Processing","title":"MIPVerify.batch_find_targeted_attack","text":"batch_find_targeted_attack(\n    nn,\n    dataset,\n    target_indices,\n    optimizer,\n    main_solve_options;\n    save_path,\n    solve_rerun_option,\n    target_labels,\n    pp,\n    norm_order,\n    tightening_algorithm,\n    tightening_options,\n    solve_if_predicted_in_targeted\n)\n\n\nRuns find_adversarial_example for the specified neural network nn and dataset for samples identified by the target_indices, with each of the target labels in target_labels individually targeted.\n\nOtherwise same parameters as batch_find_untargeted_attack.\n\n\n\n\n\n","category":"method"},{"location":"finding_adversarial_examples/batch_processing/#MIPVerify.run_on_sample_for_targeted_attack-Tuple{Integer, Integer, DataFrames.DataFrame, MIPVerify.SolveRerunOption}","page":"Batch Processing","title":"MIPVerify.run_on_sample_for_targeted_attack","text":"run_on_sample_for_targeted_attack(\n    sample_number,\n    target_label,\n    summary_dt,\n    solve_rerun_option\n)\n\n\nDetermines whether to run a solve on a sample depending on the solve_rerun_option by looking up information on the most recent completed solve recorded in summary_dt matching sample_number.\n\nsummary_dt is expected to be a DataFrame with columns :SampleNumber, :TargetIndexes, :SolveStatus, and :ObjectiveValue.\n\n\n\n\n\n","category":"method"},{"location":"finding_adversarial_examples/batch_processing/#MIPVerify.run_on_sample_for_untargeted_attack-Tuple{Integer, DataFrames.DataFrame, MIPVerify.SolveRerunOption}","page":"Batch Processing","title":"MIPVerify.run_on_sample_for_untargeted_attack","text":"run_on_sample_for_untargeted_attack(\n    sample_number,\n    summary_dt,\n    solve_rerun_option\n)\n\n\nDetermines whether to run a solve on a sample depending on the solve_rerun_option by looking up information on the most recent completed solve recorded in summary_dt matching sample_number.\n\nsummary_dt is expected to be a DataFrame with columns :SampleNumber, :SolveStatus, and :ObjectiveValue.\n\nBehavior for different choices of solve_rerun_option:\n\nnever: true if and only if there is no previous completed solve.\nalways: true always.\nresolve_ambiguous_cases: true if there is no previous completed solve, or if the   most recent completed solve a) did not find a counter-example BUT b) the optimization   was not demosntrated to be infeasible.\nrefine_insecure_cases: true if there is no previous completed solve, or if the most   recent complete solve a) did find a counter-example BUT b) we did not reach a   provably optimal solution.\n\n\n\n\n\n","category":"method"},{"location":"#MIPVerify","page":"Home","title":"MIPVerify","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MIPVerify.jl enables users to verify neural networks that are piecewise affine by: 1) finding the closest adversarial example to a selected input, or 2) proving that no adversarial example exists for some bounded family of perturbations.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"#Prerequisites","page":"Home","title":"Prerequisites","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To use our package, you require","category":"page"},{"location":"","page":"Home","title":"Home","text":"The Julia programming language\nAn optimizer supported by JuMP\nThe Julia package for working with that optimizer","category":"page"},{"location":"","page":"Home","title":"Home","text":"We recommend either Gurobi (if you have a license) or HiGHS, but any supported optimizer will work.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Platform compatibility: Julia and Gurobi are available for 32-bit and 64-bit Windows, 64-bit macOS, and 64-bit Linux, but example code in this README is for Linux.","category":"page"},{"location":"#Installing-Julia","page":"Home","title":"Installing Julia","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The latest release of this package requires version 1.6 or above of Julia. See installation instructions. To complete your installation, ensure that you are able to call julia REPL from the command line.","category":"page"},{"location":"","page":"Home","title":"Home","text":"warning: Warning\nDo not use apt-get or brew to install Julia, as the versions provided by these package managers tend to be out of date.","category":"page"},{"location":"#Installing-Gurobi","page":"Home","title":"Installing Gurobi","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Download the most recent version of the Gurobi optimizer. A license is required to use Gurobi; free academic licenses are available.","category":"page"},{"location":"#On-Ubuntu","page":"Home","title":"On Ubuntu","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"$ cd /your/path/here\n$ wget https://packages.gurobi.com/11.0/gurobi11.0.1_linux64.tar.gz\n$ tar -xvf gurobi11.0.1_linux64.tar.gz","category":"page"},{"location":"","page":"Home","title":"Home","text":"Add the following environment variables to your startup file","category":"page"},{"location":"","page":"Home","title":"Home","text":"export GUROBI_HOME=\"/your/path/here/gurobi801/linux64\"\nexport PATH=\"${PATH}:${GUROBI_HOME}/bin\"\nexport LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:${GUROBI_HOME}/lib\"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Finally, install the license obtained on a terminal prompt","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nYou will have to obtain your own license number from the Gurobi site.","category":"page"},{"location":"","page":"Home","title":"Home","text":"$ grbgetkey aaaa0000-0000-0000-0000-000000000000\n\ninfo  : grbgetkey version 8.0.1, build v8.0.1rc0\ninfo  : Contacting Gurobi key server...\ninfo  : Key for license ID 000000 was successfully retrieved\ninfo  : License expires at the end of the day on 2019-09-30\ninfo  : Saving license key...\n\nIn which directory would you like to store the Gurobi license key file?\n[hit Enter to store it in /home/ubuntu]:\n\ninfo  : License 000000 written to file /home/ubuntu/gurobi.lic","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nIf you store the license file in a non-default location, you will have to add the environment variable GRB_LICENSE_FILE to your startup file: export GRB_LICENSE_FILE=\"/your/path/here/gurobi.lic\"","category":"page"},{"location":"#Installing-Gurobi.jl","page":"Home","title":"Installing Gurobi.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Gurobi.jl is a wrapper of the Gurobi optimizer accessible in Julia. Once you have installed Gurobi and activated the license, install the latest release of Gurobi.jl:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using Pkg; Pkg.add(\"Gurobi\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can test Gurobi.jl by running","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using Pkg; Pkg.test(\"Gurobi\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"Sample output:","category":"page"},{"location":"","page":"Home","title":"Home","text":"INFO: Testing Gurobi\nAcademic license - for non-commercial use only\n...\nTest Summary: | Pass  Total\nC API         |   19     19\n...\nTest Summary:          | Pass  Total\nMathOptInterface Tests | 1415   1415\nINFO: Gurobi tests passed","category":"page"},{"location":"#Installing-MIPVerify","page":"Home","title":"Installing MIPVerify","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Once you have Julia and Gurobi installed, install the latest release of MIPVerify:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using Pkg; Pkg.add(\"MIPVerify\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can test MIPVerify by running","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using Pkg; Pkg.test(\"MIPVerify\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"These tests do take a long time to run (~30 mins), but any issues generally cause early failures.","category":"page"},{"location":"#Troubleshooting-your-installation","page":"Home","title":"Troubleshooting your installation","text":"","category":"section"},{"location":"#Invalid-Gurobi-License","page":"Home","title":"Invalid Gurobi License","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"When running Pkg.test(\"Gurobi\"):","category":"page"},{"location":"","page":"Home","title":"Home","text":"INFO: Testing Gurobi\nNo variables, no constraints: Error During Test\n  Got an exception of type ErrorException outside of a @test\n  Invalid Gurobi license\n  ...","category":"page"},{"location":"","page":"Home","title":"Home","text":"FIX: The error message indicates that you have not installed your Gurobi license. If it has been installed, the license is saved as a file gurobi.lic, typically in either the /home/ubuntu or opt/gurobi directories.","category":"page"},{"location":"#Getting-Started","page":"Home","title":"Getting Started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The best way to get started is to follow our quickstart tutorial, which demonstrates how to find adversarial examples for a pre-trained example network on the MNIST dataset. Once you're done with that, you can explore our other tutorials depending on your needs.","category":"page"}]
}
